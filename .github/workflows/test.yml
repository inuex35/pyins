name: Tests

on:
  push:
    branches: [ main, develop, master ]
  pull_request:
    branches: [ main, develop, master ]
  workflow_dispatch:  # Allow manual trigger

permissions:
  contents: read
  pull-requests: write
  checks: write
  issues: write

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, macos-latest, windows-latest]
        python-version: ["3.9", "3.10", "3.11", "3.12"]
        exclude:
          # Exclude some combinations to save CI time
          - os: macos-latest
            python-version: "3.9"
          - os: windows-latest
            python-version: "3.9"

    steps:
    - uses: actions/checkout@v5
      with:
        fetch-depth: 0  # Fetch all history for all branches and tags

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v6
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install dependencies (Unix/Linux/macOS)
      if: runner.os != 'Windows'
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest pytest-cov pytest-xdist pytest-timeout
    
    - name: Install dependencies (Windows)
      if: runner.os == 'Windows'
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest pytest-cov pytest-xdist pytest-timeout

    - name: Lint with ruff (Unix/Linux/macOS)
      if: runner.os != 'Windows'
      run: |
        pip install ruff
        # Run linting but don't fail the build
        ruff check pyins tests --output-format=github || true
        # Check formatting but don't fail the build
        ruff format --check pyins tests || true
    
    - name: Lint with ruff (Windows)
      if: runner.os == 'Windows'
      shell: pwsh
      run: |
        pip install ruff
        ruff check pyins tests --output-format=github; if ($LASTEXITCODE -ne 0) { Write-Host "Linting completed with warnings" }
        ruff format --check pyins tests; if ($LASTEXITCODE -ne 0) { Write-Host "Format check completed with warnings" }
        exit 0

    - name: Type check with mypy (Unix/Linux/macOS)
      if: runner.os != 'Windows'
      run: |
        pip install mypy types-requests
        mypy pyins --ignore-missing-imports --no-strict-optional || true
    
    - name: Type check with mypy (Windows)
      if: runner.os == 'Windows'
      shell: pwsh
      run: |
        pip install mypy types-requests
        mypy pyins --ignore-missing-imports --no-strict-optional; if ($LASTEXITCODE -ne 0) { Write-Host "Type check completed with warnings" }
        exit 0

    - name: Run tests with pytest (Unix/Linux/macOS)
      if: runner.os != 'Windows'
      run: |
        pytest tests/ --verbose --cov=pyins --cov-report=term-missing --cov-report=xml --cov-report=html --junit-xml=test-results.xml --timeout=300 -n auto
      env:
        PYTHONPATH: ${{ github.workspace }}
    
    - name: Run tests with pytest (Windows)
      if: runner.os == 'Windows'
      run: pytest tests/ --verbose --cov=pyins --cov-report=term-missing --cov-report=xml --cov-report=html --junit-xml=test-results.xml --timeout=300 -n auto
      env:
        PYTHONPATH: ${{ github.workspace }}

    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
        path: |
          test-results.xml
          htmlcov/
        retention-days: 30

    - name: Upload coverage reports to Codecov
      if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12'
      uses: codecov/codecov-action@v5
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false
        verbose: true
        token: ${{ secrets.CODECOV_TOKEN }}

    - name: Comment test results on PR (Linux)
      if: github.event_name == 'pull_request' && matrix.os == 'ubuntu-latest' && matrix.python-version == '3.12' && always()
      uses: EnricoMi/publish-unit-test-result-action@v2
      with:
        files: test-results.xml
        check_name: Test Results
        comment_title: Test Results
        fail_on: nothing
    
    - name: Comment test results on PR (macOS)
      if: github.event_name == 'pull_request' && matrix.os == 'macos-latest' && matrix.python-version == '3.12' && always()
      uses: EnricoMi/publish-unit-test-result-action/macos@v2
      with:
        files: test-results.xml
        check_name: Test Results (macOS)
        comment_title: Test Results (macOS)
        fail_on: nothing
    
    - name: Comment test results on PR (Windows)
      if: github.event_name == 'pull_request' && matrix.os == 'windows-latest' && matrix.python-version == '3.12' && always()
      uses: EnricoMi/publish-unit-test-result-action/windows@v2
      with:
        files: test-results.xml
        check_name: Test Results (Windows)
        comment_title: Test Results (Windows)
        fail_on: nothing

  benchmark:
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v5
    
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.12'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark
    
    - name: Run benchmarks
      run: |
        pytest tests/ --benchmark-only --benchmark-json=benchmark.json || true
    
    - name: Check and create benchmark file if needed
      run: |
        if [ ! -f benchmark.json ] || [ ! -s benchmark.json ]; then
          echo "Creating valid empty benchmark JSON"
          cat > benchmark.json << 'EOF'
        {
          "version": "1.0.0",
          "machine_info": {
            "node": "github-actions",
            "processor": "unknown",
            "machine": "unknown",
            "python_compiler": "GCC",
            "python_implementation": "CPython",
            "python_version": "3.12.0",
            "python_build": ["default", ""],
            "release": "unknown",
            "system": "Linux",
            "cpu": {}
          },
          "commit_info": {
            "id": "unknown",
            "time": "unknown",
            "author_time": "unknown",
            "dirty": false,
            "project": "pyins",
            "branch": "main"
          },
          "benchmarks": []
        }
        EOF
        fi
        echo "Benchmark file content:"
        cat benchmark.json | head -30
    
    - name: Check if benchmarks exist
      id: check-benchmarks
      run: |
        if [ -f benchmark.json ] && [ -s benchmark.json ]; then
          BENCHMARK_COUNT=$(python -c "import json; data = json.load(open('benchmark.json')); print(len(data.get('benchmarks', [])))")
          echo "Found $BENCHMARK_COUNT benchmarks"
          if [ "$BENCHMARK_COUNT" -gt "0" ]; then
            echo "has_benchmarks=true" >> $GITHUB_OUTPUT
          else
            echo "has_benchmarks=false" >> $GITHUB_OUTPUT
          fi
        else
          echo "has_benchmarks=false" >> $GITHUB_OUTPUT
        fi
    
    - name: Store benchmark result
      uses: benchmark-action/github-action-benchmark@v1
      if: steps.check-benchmarks.outputs.has_benchmarks == 'true'
      with:
        tool: 'pytest'
        output-file-path: benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '150%'
        comment-always: false
        save-data-file: true

  security:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v5
    
    - name: Set up Python
      uses: actions/setup-python@v6
      with:
        python-version: '3.12'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install bandit[toml] safety
    
    - name: Security check with bandit
      run: bandit -r pyins -f json -o bandit-report.json || true
    
    - name: Check for known vulnerabilities
      run: |
        pip install -e .
        safety check --json || true
    
    - name: Upload security results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: security-results
        path: bandit-report.json
        retention-days: 30